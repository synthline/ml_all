{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "8d88a0a3",
            "metadata": {},
            "source": [
                "# Regularization of Linear Models with SKLearn\n",
                "This exercise is about regularization. The first part will be using SKLearn. \n",
                " You have earlier in this course seen that increasing the order of a polynomial may result in overfitting. This exercise is about investigating the impact on regularization and how this relates to polynomial fitting of data.   \n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "da34899c",
            "metadata": {},
            "source": [
                "Let\u2019s import the necessary libraries and load the training dataset.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 15,
            "id": "3031c478",
            "metadata": {},
            "outputs": [],
            "source": [
                "#imports\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import math\n",
                "import warnings\n",
                "\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "from sklearn.linear_model import LinearRegression\n",
                "from sklearn.linear_model import Ridge\n",
                "from sklearn.linear_model import Lasso\n",
                "from sklearn.linear_model import ElasticNet\n",
                "from sklearn.metrics import mean_squared_error\n",
                "\n",
                "from sklearn.preprocessing import PolynomialFeatures\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "sns.set()\n",
                "%matplotlib inline\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "eb5e9655",
            "metadata": {},
            "source": [
                "The next step is to split the dataset into a training set and a validation set. 30% of the data will be used for validation. You will pass an int to \"random_state\" in \"train_test_split\" function for reproducible output across multiple function calls.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "id": "3fbc6d20",
            "metadata": {},
            "outputs": [],
            "source": [
                "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
                "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
                "X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
                "y = raw_df.values[1::2, 2]\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.3)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "f519d90b",
            "metadata": {},
            "source": [
                "This exercise uses a linear regression model as baseline.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 17,
            "id": "14aecf7a",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Training score: 0.7434997532004697\n",
                        "Test score: 0.7112260057484974\n",
                        "RMSE_train: 4.748208239685937\n",
                        "RMSE_test: 4.638689926172788\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "lr_model = LinearRegression()\n",
                "lr_model.fit(X_train, y_train)\n",
                "\n",
                "print('Training score: {}'.format(lr_model.score(X_train, y_train)))\n",
                "print('Test score: {}'.format(lr_model.score(X_test, y_test)))\n",
                "\n",
                "y_pred_train = lr_model.predict(X_train)\n",
                "mse_train= mean_squared_error(y_train, y_pred_train)\n",
                "rmse_train = math.sqrt(mse_train)\n",
                "\n",
                "print('RMSE_train: {}'.format(rmse_train))\n",
                "\n",
                "y_pred_test = lr_model.predict(X_test)\n",
                "mse_test = mean_squared_error(y_test, y_pred_test)\n",
                "rmse_test = math.sqrt(mse_test)\n",
                "\n",
                "print('RMSE_test: {}'.format(rmse_test))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d9bcb02a",
            "metadata": {},
            "source": [
                "The linear model obtains a training accuracy and a test accuracy around 72%-74% and an RMSE of about 4.5. \n",
                "\n",
                "The next step is to fit the data to the models using the \"steps\" within a  pipeline by scaling the data, then create polynomial models, and then train a linear regression model.\n",
                "\n",
                "The first step is to normalize the inputs by  mean centering and scaling to unit variance. This serves the purpose of letting us work with reasonable numbers when we raise to a power. \n",
                "\n",
                "\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 18,
            "id": "ea962485",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "RMSE_train: 2.162970056950185\n",
                        "RMSE_test: 5.0811876783255245\n",
                        "\n",
                        "Training score: 0.9467733311147442\n",
                        "Test score: 0.6535042863861226\n"
                    ]
                }
            ],
            "source": [
                "steps = [\n",
                "    ('scalar', StandardScaler()),\n",
                "    ('poly', PolynomialFeatures(degree=2)),\n",
                "    ('model', LinearRegression())\n",
                "]\n",
                "\n",
                "pipeline = Pipeline(steps)\n",
                "\n",
                "pipeline.fit(X_train, y_train)\n",
                "\n",
                "y_pred_train = pipeline.predict(X_train)\n",
                "mse_train= mean_squared_error(y_train, y_pred_train)\n",
                "rmse_train = math.sqrt(mse_train)\n",
                "print('RMSE_train: {}'.format(rmse_train))\n",
                "\n",
                "y_pred_test = pipeline.predict(X_test)\n",
                "mse_test = mean_squared_error(y_test, y_pred_test)\n",
                "rmse_test = math.sqrt(mse_test)\n",
                "print('RMSE_test: {}\\n'.format(rmse_test))\n",
                "\n",
                "print('Training score: {}'.format(pipeline.score(X_train, y_train)))\n",
                "print('Test score: {}'.format(pipeline.score(X_test, y_test)))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "540614ce",
            "metadata": {},
            "source": [
                "After running the code, you will get a training accuracy of about 94%, and a test accuracy of 65%. This is a sign of overfitting. It is normally not a desirable feature, but that is exactly what we were hoping for this example. \n",
                "\n",
                "You will now apply regularization to the data.\n",
                "## l2 Regularization or Ridge Regression\n",
                " Recall what happens when the model coefficients are learned during gradient descent. The weights are updated  using the learning rate and the gradient as mentioned in the lecture about non-linear optimization. Ridge regression adds a penalty term in objective function,\n",
                "\n",
                "${\\begin{align*}\\frac{1}{2} \\sum_{n=1}^{N}\\left\\{y_{n}-\\theta^{\\top} \\boldsymbol{\\phi}\\left(\\mathbf{x}_{n}\\right)\\right\\}^{2}+\\frac{\\alpha}{2}\\|\\theta\\|_2^2\n",
                "\\end{align*}}$\n",
                "\n",
                " The importance of the regularization  term, can be tuned by changing $\\alpha$. The larger the value of $\\alpha$, the less variance (variability of model prediction for a given data point)\n",
                "your model will exhibit.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "id": "5778e738",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "RMSE_train: 2.441071076959751\n",
                        "RMSE_test: 3.823376123713985\n",
                        "Training Score: 0.9322063334864212\n",
                        "Test Score: 0.8038169683868278\n"
                    ]
                }
            ],
            "source": [
                "steps = [\n",
                "    ('scalar', StandardScaler()),\n",
                "    ('poly', PolynomialFeatures(degree=2)),\n",
                "    ('model', Ridge(alpha=10, fit_intercept=True))\n",
                "]\n",
                "\n",
                "ridge_pipe = Pipeline(steps)\n",
                "ridge_pipe.fit(X_train, y_train)\n",
                "\n",
                "y_pred_train = ridge_pipe.predict(X_train)\n",
                "mse_train= mean_squared_error(y_train, y_pred_train)\n",
                "rmse_train = math.sqrt(mse_train)\n",
                "print('RMSE_train: {}'.format(rmse_train))\n",
                "\n",
                "y_pred_test = ridge_pipe.predict(X_test)\n",
                "mse_test = mean_squared_error(y_test, y_pred_test)\n",
                "rmse_test = math.sqrt(mse_test)\n",
                "print('RMSE_test: {}'.format(rmse_test))\n",
                "\n",
                "print('Training Score: {}'.format(ridge_pipe.score(X_train, y_train)))\n",
                "print('Test Score: {}'.format(ridge_pipe.score(X_test, y_test)))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2e643082",
            "metadata": {},
            "source": [
                "The regression model achives a training accuracy of about 92%, and a test accuracy of about 80%. That is an improvement compared to the baseline linear regression model.\n",
                "\n",
                "## l1 Regularization or Lasso Regression\n",
                "A a pipeline similarly to the Ridge regression example is created, but this time using Lasso. The objective function for Lasso regression is using the 1-norm on the parameters.\n",
                "\n",
                "${\\frac{1}{2} \\sum_{n=1}^{N}\\left\\{y_{n}-\\theta^{\\top} \\boldsymbol{\\phi}\\left(\\mathbf{x}_{n}\\right)\\right\\}^{2}+\\frac{\\alpha}{2}\\|\\theta\\|_1 }$\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 20,
            "id": "ce3f0b2e",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "RMSE_train: 3.538738418298479\n",
                        "RMSE_test: 3.970165571442558\n",
                        "Training score: 0.8575294192309941\n",
                        "Test score: 0.7884638325042947\n"
                    ]
                }
            ],
            "source": [
                "steps = [\n",
                "    ('scalar', StandardScaler()),\n",
                "    ('poly', PolynomialFeatures(degree=2)),\n",
                "    ('model', Lasso(alpha=0.3, fit_intercept=True))\n",
                "]\n",
                "\n",
                "lasso_pipe = Pipeline(steps)\n",
                "\n",
                "lasso_pipe.fit(X_train, y_train)\n",
                "\n",
                "y_pred_train = lasso_pipe.predict(X_train)\n",
                "mse_train= mean_squared_error(y_train, y_pred_train)\n",
                "rmse_train = math.sqrt(mse_train)\n",
                "print('RMSE_train: {}'.format(rmse_train))\n",
                "\n",
                "y_pred_test = lasso_pipe.predict(X_test)\n",
                "mse_test = mean_squared_error(y_test, y_pred_test)\n",
                "rmse_test = math.sqrt(mse_test)\n",
                "print('RMSE_test: {}'.format(rmse_test))\n",
                "\n",
                "print('Training score: {}'.format(lasso_pipe.score(X_train, y_train)))\n",
                "print('Test score: {}'.format(lasso_pipe.score(X_test, y_test)))\n"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "89a7a197",
            "metadata": {},
            "source": [
                "# tasks :\n",
                "In Exercise week 9 task 3, you were supposed to find the optimal  polynomial model. In this task you have to use that polynomial model and then extend it with ridge and and lasso regression:\n",
                "1. Calculate the RMSE of the training and test sets (as you did in Exercise week 9) for the \"optimal\" polynomial\n",
                "2. Use  **ridge regression** to estimate a polynomial of degree 10 and and calculate the RMSE on the trainig and test sets\n",
                "3. Use  the model in question 2 to find the optimal $\\alpha$ (around 0.0001) using RMSE\n",
                "4. Use the same polynomial but this time  apply **Lasso regression** to find the optimal value of $\\alpha$ (around 0.001) using  RMSE on the training and test sets.\n",
                "5. The experiments shows that regularization (ridge regression and Lasso regression)  performs better when using the RMSE on the test set. You remember that higher degrees terms (more complexity) lead the model to overfitting. Thus, Why does the regularized model (which uses polynimial with higher degree) perform better on test set(better from overfitting view)?  \n",
                "\n",
                "Optimal model Polynomial:\n",
                "$w_0+w_1X+w_2X^2+w_3X^3$\n",
                "\n",
                "Regulrized model Polynomial:\n",
                "$wr_0+wr_1X+wr_2X^2+wr_3X^3+...+wr_{10}X^{10}$\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 21,
            "id": "32402b0e",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Train/Test Size :  (18,) (12,) (18,) (12,)\n",
                        "RMSE_train: 0.056532247455264445\n",
                        "RMSE_test: 0.30856057840599427\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "#use this code for your solutions \n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "#cosin function\n",
                "def true_fun(X):\n",
                "    return np.cos(1.5 * np.pi * X)\n",
                "\n",
                "np.random.seed(0)\n",
                "\n",
                "n_samples = 30\n",
                "\n",
                "X = np.sort(np.random.rand(n_samples))\n",
                "y = true_fun(X) + np.random.randn(n_samples) * 0.1\n",
                "\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.60, test_size=0.40, random_state=1)\n",
                "print('Train/Test Size : ', X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
                "degree=10\n",
                "polynomial_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
                "linear_regression = LinearRegression()\n",
                "pipeline = Pipeline(\n",
                "    [\n",
                "        (\"polynomial_features\", polynomial_features),\n",
                "        (\"linear_regression\", linear_regression),\n",
                "    ]\n",
                ")\n",
                "pipeline.fit(X_train[:, np.newaxis], y_train)\n",
                "\n",
                "y_pred_train = pipeline.predict(X_train[:, np.newaxis])\n",
                "mse_train= mean_squared_error(y_train, y_pred_train)\n",
                "rmse_train = math.sqrt(mse_train)\n",
                "print('RMSE_train: {}'.format(rmse_train))\n",
                "\n",
                "y_pred_test = pipeline.predict(X_test[:, np.newaxis])\n",
                "mse_test = mean_squared_error(y_test, y_pred_test)\n",
                "rmse_test = math.sqrt(mse_test)\n",
                "print('RMSE_test: {}\\n'.format(rmse_test))\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 36,
            "id": "1d8618c2",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Train/Test Size :  (18,) (12,) (18,) (12,)\n",
                        "RMSE_train: 0.1079839948207335\n",
                        "RMSE_test: 0.12863714132664852\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Copy paste your  code here\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "id": "65230244",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "RMSE_train: 0.10126814910229279\n",
                        "RMSE_test: 0.11689857032619674\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Copy paste your  code here\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "id": "9c958a34",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "RMSE_train: 0.10041317401921453\n",
                        "RMSE_test: 0.11953621236626978\n",
                        "\n"
                    ]
                }
            ],
            "source": [
                "# Copy paste your code here\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1380df8f",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}