{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde69e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efb6825",
   "metadata": {
    "tags": []
   },
   "source": [
    "# <i class=\"fas fa-circle\" style=\"color: #f0ad4e;\"></i> Introduction to PCA\n",
    "\n",
    "In this exercise you will create a basic PCA implementation that you will use for the next exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27535b5c",
   "metadata": {},
   "source": [
    "## Implementing PCA\n",
    "\n",
    "Your first task is to implement the PCA method as well as functions for\n",
    "transforming to and from the space defined by the principal components.\n",
    "But first, a quick recap of the terminology to minimize confusion.\n",
    "\n",
    "Principal component analysis is about finding a linear transformation\n",
    "that reduces the number of dimensions used to represent samples while\n",
    "destroying as little of the variation as possible. PCA is defined by\n",
    "$\\Phi_{:k}$, an $M\\times k$ matrix representing a linear transformation from\n",
    "vectors in $M$-dimensional real space to $k$-dimensional latent space. We have the\n",
    "following transformations\n",
    "\n",
    "$$ \n",
    "b = \\Phi_{:k}^\\top x, \n",
    "$$\n",
    "\n",
    "$$\n",
    "x + \\epsilon = \\Phi_{:k} b,\n",
    "$$\n",
    "\n",
    "where\n",
    "$x\\in\\mathbb{R}^M$ is the input vector and $y\\in\\mathbb{R}^K$ is the\n",
    "embedded vector. As shown in the second equation, it is possible to reconstruct $x$ with some amount of error $\\epsilon$. To find $\\Phi$, we use the _eigenvectors_ of the covariance matrix of our data matrix $W$ where each row $i$ is a sample $x_i \\in \\mathbb{R}^M$. The eigenvectors are sorted by their associated eigenvalues which represent the variance of each dimension in latent space. Selecting the $k$ first columns (we use the notation $\\Phi_{:k}$) results in a transformation that reduces the dimensionality of the latent space to $k$ dimensions. \n",
    "\n",
    "1.  <i class=\"fas fa-code\"></i> **Setup:** Create a script file or notebook for this exercise. In it, start by loading the face shapes and images using `utils.face_shape_data`.\n",
    "\n",
    "2.  <i class=\"fas fa-code\"></i> **Implement PCA:** Create a function that calculates and returns the\n",
    "    principle components of the shapes dataset. Use the method described\n",
    "    above where the eigenvectors of the covariance matrix is used.\n",
    "    **Make sure to center the samples (subtract the mean before\n",
    "    calculating the covariance matrix)**.\n",
    "\n",
    "3.  <i class=\"fas fa-code\"></i> **Implement transformations:** Create two functions, one for\n",
    "    transforming from feature space to principal component space\n",
    "    (eqaution {eq}`trans`) and one for transforming from principal\n",
    "    component space to feature space\n",
    "    (equation {eq}`inv`). You have to subtract the $\\mu$ vector when\n",
    "    transforming to the principal component space and add it again when\n",
    "    transforming back to feature space. You may use the following\n",
    "    modified equations for reference:\n",
    "\n",
    "$$\n",
    "b = \\Phi_{:k}^\\top(x-\\mu)\n",
    "$$  \n",
    "\n",
    "$$\n",
    "x = \\Phi_{:k} b + \\mu\n",
    "$$\n",
    "\n",
    "```{tip}\n",
    "The reading material for the PCA lecture contains an excellent [tutorial](https://sebastianraschka.com/Articles/2014_pca_step_by_step.html) on how this can be done, but remember that copying is not allowed!!\n",
    "```\n",
    "\n",
    "```{tip}\n",
    "Some of the later tasks will be easier if you return all 146 principle components. You can then create another function for extracting $n$ components to generate $\\Phi$.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b95aad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your implementation here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd493c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "path = './db'\n",
    "shapes, images = face_shape_data(path)\n",
    "\n",
    "# Write your implementation here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc96f3a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluating precision\n",
    "\n",
    "As described above, using PCA to transform a sample $x$ to a principal\n",
    "component space and back again likely results in an error $\\epsilon$, called the _reconstruction error_. In this task you will implement a\n",
    "method for calculating this error and use it to test the effect of\n",
    "increasing or decreasing the number of principal components used.\n",
    "\n",
    "When solving regression problems, the error is typically measured as the\n",
    "average distance error, otherwise known as root mean square error\n",
    "(RMSE). This is also used when calculating the construction error. For\n",
    "reference, the RMSE is\n",
    "\n",
    "$$RMSE(x, \\widetilde{x}) = \\sqrt{\\frac{1}{N}\\sum_i (x_i-\\widetilde{x}_i)^2},$$\n",
    "\n",
    "where $x$, $\\widetilde{x}$ are the original and transformed samples\n",
    "respectively and $N$ is the total number of samples $x_i$.\n",
    "\n",
    "Another method for evaluating PCA models is to look at the eigenvalues,\n",
    "where eigenvalue $i$ is denoted $\\lambda^{(i)}$. The eigenvalues explain\n",
    "the variance of each dimension when that data has been transformed by\n",
    "PCA. The sum of all eigenvalues $\\lambda^{(1)}+\\dots+\\lambda^{(n)}$ is\n",
    "equal to the total variance of the data. By comparing all the\n",
    "eigenvalues we can calculate:\n",
    "\n",
    "(1) **Proportional variance:** What proportion of the total variance is\n",
    "explained by a single component. The following formula can be used\n",
    "\n",
    "$$\\frac{\\lambda^{(i)}}{\\lambda^{(1)} + \\dots + \\lambda^{(n)}}$$\n",
    "\n",
    "(2) **Cumulative proportional variance:** What cumulative proportion of\n",
    "the total variance is explained by the first $k$ components.\n",
    "\n",
    "$$\\frac{\\lambda^{(1)} + \\dots + \\lambda^{(k)}}{\\lambda^{(1)} + \\dots + \\lambda^{(n)}}$$\n",
    "\n",
    "\n",
    "1.  **<i class=\"fas fa-code\"></i> Calculate reconstruction error:** Implement a function in your\n",
    "    script that calculates the reconstruction error given a dataset $X$,\n",
    "    principle components $\\Phi$, and a mean vector $\\mu$.\n",
    "\n",
    "2.  **<i class=\"fas fa-code\"></i> Plot reconstruction error:** When constructing $\\Phi$ you may use a single principal component or all of them. Plot the reconstruction error of $\\Phi$ for all possible numbers of principle components. An example is shown in {numref}`fig:plot_var`.\n",
    "\n",
    "3.  **<i class=\"fas fa-code\"></i> Calculate variance:** Create functions that calculate the\n",
    "    _proportional_ and _cumulative proportional_ variance.\n",
    "\n",
    "4.  **<i class=\"fas fa-code\"></i> Plot variance metrics:** Plot both the proportional and cumulative\n",
    "    proportional variance in a single plot. An example is shown in\n",
    "    {numref}`fig:plot_var`.\n",
    "\n",
    "\n",
    "```{figure} ./img/plot_var.png\n",
    "---\n",
    "name: fig:plot_var\n",
    "width: 400px\n",
    "---\n",
    "The expected result of the cumulative and individual variance\n",
    "proportion.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527fab68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your implementation here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad4251e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your implementation here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39285cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
